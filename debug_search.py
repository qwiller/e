#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è°ƒè¯•å‘é‡æœç´¢åŠŸèƒ½
"""

import sys
import os
import logging

# è®¾ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

def debug_vector_search():
    """
    è°ƒè¯•å‘é‡æœç´¢åŠŸèƒ½
    """
    print("ğŸ” è°ƒè¯•å‘é‡æœç´¢åŠŸèƒ½")
    print("=" * 50)
    
    try:
        from src.vector_store import VectorStore
        from config import VECTOR_CONFIG
        
        # åˆ›å»ºå‘é‡å­˜å‚¨å®ä¾‹
        print("1. åˆ›å»ºå‘é‡å­˜å‚¨å®ä¾‹...")
        vector_store = VectorStore()
        
        # æ˜¾ç¤ºå½“å‰çŠ¶æ€
        stats = vector_store.get_stats()
        print(f"   æ•°æ®åº“è·¯å¾„: {stats['db_path']}")
        print(f"   æ–‡æ¡£æ•°é‡: {stats['document_count']}")
        print(f"   æ˜¯å¦å·²è®­ç»ƒ: {stats['is_fitted']}")
        print(f"   å‘é‡å½¢çŠ¶: {stats['vector_shape']}")
        
        # å¦‚æœæ²¡æœ‰æ–‡æ¡£ï¼Œæ·»åŠ æµ‹è¯•æ–‡æ¡£
        if stats['document_count'] == 0:
            print("\n2. æ·»åŠ æµ‹è¯•æ–‡æ¡£...")
            test_docs = [
                {
                    'content': 'å¤©æ´¥å•†åŠ¡èŒä¸šå­¦é™¢åœ¨ç¬¬åäº”å±Šå…¨å›½èŒä¸šé™¢æ ¡æŠ€èƒ½å¤§èµ›ä¸­è·å¾—ä¼˜å¼‚æˆç»©ï¼Œè£è·å¤šé¡¹å¥–é¡¹ã€‚å­¦é™¢åœ¨è®¡ç®—æœºåº”ç”¨æŠ€æœ¯ã€ç”µå­å•†åŠ¡ã€ä¼šè®¡ç­‰ä¸“ä¸šé¢†åŸŸè¡¨ç°çªå‡ºã€‚',
                    'source': 'test_award.txt',
                    'metadata': {'type': 'award', 'school': 'å¤©æ´¥å•†åŠ¡èŒä¸šå­¦é™¢'}
                },
                {
                    'content': 'ç¬¬åäº”å±Šå…¨å›½èŒä¸šé™¢æ ¡æŠ€èƒ½å¤§èµ›è·å¥–åå•å…¬å¸ƒï¼Œå¤©æ´¥å•†åŠ¡èŒä¸šå­¦é™¢å­¦ç”Ÿåœ¨å¤šä¸ªé¡¹ç›®ä¸­è·å¾—ä¸€ç­‰å¥–ã€äºŒç­‰å¥–å’Œä¸‰ç­‰å¥–ã€‚',
                    'source': 'award_list.txt',
                    'metadata': {'type': 'competition', 'event': 'ç¬¬åäº”å±Šå¤§èµ›'}
                }
            ]
            
            vector_store.add_documents(test_docs)
            stats = vector_store.get_stats()
            print(f"   æ·»åŠ åæ–‡æ¡£æ•°é‡: {stats['document_count']}")
        
        # æ˜¾ç¤ºæ‰€æœ‰æ–‡æ¡£å†…å®¹
        print("\n3. å½“å‰å­˜å‚¨çš„æ–‡æ¡£:")
        for i, doc in enumerate(vector_store.documents):
            print(f"   æ–‡æ¡£ {i}: {doc.get('content', '')[:100]}...")
        
        # æµ‹è¯•ä¸åŒçš„æŸ¥è¯¢
        print("\n4. æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½:")
        test_queries = [
            "å¤©æ´¥å•†åŠ¡èŒä¸šå­¦é™¢è·å¥–æƒ…å†µ",
            "ç¬¬åäº”å±Šå¤§èµ›",
            "èŒä¸šé™¢æ ¡æŠ€èƒ½å¤§èµ›",
            "è·å¥–åå•",
            "å¤©æ´¥å•†åŠ¡",
            "è®¡ç®—æœºåº”ç”¨"
        ]
        
        print(f"å½“å‰é…ç½®: ç›¸ä¼¼åº¦é˜ˆå€¼={VECTOR_CONFIG.get('similarity_threshold', 0.1)}")
        
        for query in test_queries:
            print(f"\n   æŸ¥è¯¢: '{query}'")
            
            # ä½¿ç”¨è°ƒè¯•æ¨¡å¼æœç´¢
            results = vector_store.search(query, top_k=5)
            
            print(f"   ç»“æœæ•°é‡: {len(results)}")
            for j, result in enumerate(results):
                similarity = result.get('similarity', 0)
                content = result.get('content', '')[:80]
                print(f"     [{j+1}] ç›¸ä¼¼åº¦: {similarity:.4f} - {content}...")
        
        # æµ‹è¯•ä¸åŒé˜ˆå€¼
        print("\n5. æµ‹è¯•ä¸åŒç›¸ä¼¼åº¦é˜ˆå€¼:")
        test_query = "å¤©æ´¥å•†åŠ¡èŒä¸šå­¦é™¢è·å¥–æƒ…å†µ"
        
        original_threshold = VECTOR_CONFIG['similarity_threshold']
        
        for threshold in [0.0, 0.05, 0.1, 0.2, 0.3]:
            VECTOR_CONFIG['similarity_threshold'] = threshold
            results = vector_store.search(test_query, top_k=3)
            print(f"   é˜ˆå€¼ {threshold}: {len(results)} ä¸ªç»“æœ")
            
            for result in results:
                print(f"     ç›¸ä¼¼åº¦: {result.get('similarity', 0):.4f}")
        
        # æ¢å¤åŸå§‹é˜ˆå€¼
        VECTOR_CONFIG['similarity_threshold'] = original_threshold
        
        print("\n6. æ£€æŸ¥å‘é‡åŒ–å™¨:")
        if hasattr(vector_store.vectorizer, 'vocabulary_'):
            vocab_size = len(vector_store.vectorizer.vocabulary_)
            print(f"   è¯æ±‡è¡¨å¤§å°: {vocab_size}")
            
            # æ˜¾ç¤ºä¸€äº›è¯æ±‡
            vocab_items = list(vector_store.vectorizer.vocabulary_.items())[:10]
            print(f"   è¯æ±‡ç¤ºä¾‹: {vocab_items}")
        
        # æµ‹è¯•æŸ¥è¯¢å‘é‡åŒ–
        print("\n7. æµ‹è¯•æŸ¥è¯¢å‘é‡åŒ–:")
        test_query = "å¤©æ´¥å•†åŠ¡èŒä¸šå­¦é™¢"
        query_vector = vector_store.vectorizer.transform([test_query])
        print(f"   æŸ¥è¯¢: '{test_query}'")
        print(f"   å‘é‡å½¢çŠ¶: {query_vector.shape}")
        print(f"   éé›¶å…ƒç´ : {query_vector.nnz}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_jieba_tokenization():
    """
    æµ‹è¯•jiebaåˆ†è¯åŠŸèƒ½
    """
    print("\nğŸ”¤ æµ‹è¯•jiebaåˆ†è¯åŠŸèƒ½")
    print("=" * 50)
    
    try:
        import jieba
        
        test_texts = [
            "å¤©æ´¥å•†åŠ¡èŒä¸šå­¦é™¢è·å¥–æƒ…å†µ",
            "ç¬¬åäº”å±Šå…¨å›½èŒä¸šé™¢æ ¡æŠ€èƒ½å¤§èµ›",
            "è®¡ç®—æœºåº”ç”¨æŠ€æœ¯ä¸“ä¸š"
        ]
        
        for text in test_texts:
            tokens = list(jieba.cut(text))
            print(f"åŸæ–‡: {text}")
            print(f"åˆ†è¯: {tokens}")
            print()
        
        return True
        
    except Exception as e:
        print(f"âŒ jiebaæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸ” å‘é‡æœç´¢è°ƒè¯•å·¥å…·")
    print("=" * 50)
    
    try:
        # æµ‹è¯•jiebaåˆ†è¯
        test_jieba_tokenization()
        
        # è°ƒè¯•å‘é‡æœç´¢
        debug_vector_search()
        
        print("\nğŸ‰ è°ƒè¯•å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nğŸ‘‹ è°ƒè¯•ä¸­æ–­")
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
